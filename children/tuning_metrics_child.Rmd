---
title: "tuning_metrics_child"
output: html_document
---

# Tuning and Metrics {.tabset}

Workflowsets are first tuned in the code snippet below:

```{r tune_workflows_cv, message=FALSE,cache = params$run_cache, class.source = 'fold-show', eval=params$n_folds > 1}
# browser()
#Run workflow set through cross validation ----
source("scripts/tune_cv.R",echo = TRUE, local = knitr::knit_global())



#t<-extract_workflow_set_result(tuned_workflows, "mainall_rf")

# autoplot(tuned_workflows)
# tuned_workflows[6,][["result"]][[1]][[".notes"]]

# Save tuned workflow to S3
# if (params$save_results){
#   WriteToS3(df = tuned_workflows,
#             s3name = paste0(params$run_name, "_tuned"),
#             type = "RDS",
#             obj = data_directory)
# }

```

```{r stacking, warning=FALSE,cache = params$run_cache, eval = params$run_stack}
# Combine mdoels with stacks
# https://stacks.tidymodels.org/articles/basics.html
#https://stacks.tidymodels.org/articles/workflowsets.html
# https://www.youtube.com/watch?v=HZXZxvdpDOg&t=2160s
df_wf_set_stack<-
  #Initiliaze the stacks
  stacks()%>%
  add_candidates(tuned_workflows)%>%
  # determine how to combine their predictions
  # Look deeper into setting up a meta model
  # include fewer models by proposing higher penalties
  blend_predictions(penalty = 10^seq(-2, -0.5, length = 20))%>%
  # fit the candidates with nonzero stacking coefficients
  fit_members()

# Relevant when tuning:
# collect_parameters(df_wf_set_stack, "rf")

# TODO: Save stacked model to S3

```

Tuned models are saved [HERE] on S3.

## Tuned Models

```{r rank_results, message=FALSE,warning=FALSE, fig.width=12,out.width="100%", dpi=300,eval=params$n_folds > 1}
ranked_results<-
  rank_results(tuned_workflows)%>%
    tidyr::extract(wflow_id, c("recipe", "model"), "([^_]+)_(.*)")


ranked_results%>%
  ggplot(aes(x = rank, y = mean, colour = model, shape = recipe))+
  geom_point()+
  geom_errorbar(aes(ymin = mean - std_err,
                    ymax = mean + std_err))+
  scale_colour_iwanthue() +
  scale_x_continuous(breaks= pretty_breaks())+
  labs(y="")+
  facet_wrap(~.metric, scales = "free")

# Plot of predictions from each fold
collect_predictions(tuned_workflows, summarize = F)%>%
  ggplot(aes(y = .pred, x = !!sym(outcome_var), colour = id))+
  geom_point(alpha = 0.1)+
  facet_wrap(~wflow_id)+
  scale_colour_iwanthue()+
  guides(colour = guide_legend(override.aes = list(alpha=1)))+
  labs(colour = "")

# V2
# collect_predictions(tuned_workflows, summarize = F)%>%
#   ggplot(aes(y = .pred, x = !!sym(outcome_var), colour = id))+
#   geom_point(alpha = 0.1)+
#   facet_grid(wflow_id~id)+
#   scale_colour_iwanthue()+
#   guides(colour = guide_legend(override.aes = list(alpha=1)))



FormatReactable(rank_results(tuned_workflows))
ensemble_predictions<- collect_predictions(tuned_workflows)

```

## Stacking

After each model is trained, we can generate an ensemble model via stacking:

```{r stacking_summary, warning=FALSE, fig.show="hold", fig.width=10,out.width="50%", dpi=300,eval = params$run_stack}
print(df_wf_set_stack)


autoplot(df_wf_set_stack)
# autoplot(df_wf_set_stack, type = "weights")

ensemble_preds<-
  df_test%>%
  mutate(predict(df_wf_set_stack, .))%>%
  mutate(ensemble_error = !!sym(outcome_var) - .pred)

member_preds_test <-
  df_test %>%
  dplyr::select(!!sym(outcome_var))%>%
  # Include all members as an additional
  bind_cols(predict(df_wf_set_stack, df_test, members = TRUE))

# Member predictions compared to ensemble
member_stats_test<-
  map_dfr(colnames(member_preds_test), rmse, truth = !!sym(outcome_var), data = member_preds_test) %>%
  mutate(member = colnames(member_preds_test))


# On training data
member_preds_train<-
  df_train %>%
  dplyr::select(!!sym(outcome_var))%>%
  # Include all members as an additional
  bind_cols(predict(df_wf_set_stack, df_train, members = TRUE))

member_stats_train<-
  map_dfr(colnames(member_preds_train), rmse, truth = !!sym(outcome_var), data = member_preds_train) %>%
  mutate(member = colnames(member_preds_train))


ensemble_preds%>%
  ggplot(aes(x = !!sym(outcome_var), y = .pred))+
  geom_point()+
  geom_abline(linetype = "dashed",
              color = "gray")+
  labs(x = "actual", y = "predicted", title = "Stacking Test Actual vs Predicted", subtitle= paste0("(RMSE: ", round(member_stats_test%>%filter(member == ".pred")%>%pull(.estimate), 2), ")"))+
  theme_bw()


FormatReactable(member_stats_test)
```

```{r tune_extract_individual, warning=FALSE, cache = params$run_cache, cache.lazy = FALSE}

# Tune individual workflows
source("scripts/tune_workflows.R",echo = TRUE, local = knitr::knit_global())


#example use:
#fits[['main_rf']][[2]]
# browser()

if (params$save_results){
  saveRDS(fits[[params$model_to_save]][[1]], paste0("~/downloads/", params$run_name,"_", params$model_to_save,"_fit",".RDS"))
  
  # WriteToS3(df = fits[[params$model_to_save]][[1]]%>%extract_fit_parsnip(),
  #           s3name = paste0(params$run_name,"_", params$model_to_save,"_fit"),
  #           type = "RDS",
  #           obj = data_directory)
}


```

## RMSE Summary

Below is a summary of RMSEs across the training data, Cross-validated model set, and test set for all **`r nrow(df_wf_set)`** models plus the ensemble.

```{r rmse_summary, warning=FALSE}
rmses<-
  lapply(df_wf_set$wflow_id, function(e){

  tibble(m = e,
         test_rmse = fits[[e]][[3]],
         train_rmse = fits[[e]][[4]]
         )
})%>%
  bind_rows()

if(params$n_folds > 1){
  rmses<-
    rmses%>%
    left_join(
      ranked_results%>%
        mutate(m = paste0(recipe,"_", model))%>%
        filter(.metric == "rmse")%>%
        dplyr::select(m, "cv_rmse" = mean),
      by = "m"
    )
}

if(params$run_stack){
  rmses<-
    rmses %>%
  bind_rows(member_stats_test%>%
              filter(member == ".pred")%>%
              transmute(m = "ensemble", test_rmse = .estimate)%>%
              left_join(member_stats_train%>%
                          filter(member == ".pred")%>%
                          transmute(m = "ensemble",
                                    train_rmse = .estimate), by = "m")
            )%>%
  arrange(test_rmse)%>%
  mutate(rank = 1:n())
}

FormatReactable(rmses)

```

Below is a grid of how the errors across models are correlated
```{r crossmodel_error, warning=FALSE,fig.width=12,out.width="100%", dpi=300}

error_crossmodel<-
  lapply(df_wf_set$wflow_id, function(e){
    fits[[e]][[8]]%>%
      dplyr::select(.pred, error, model, !!sym(outcome_var))%>%
      mutate(wflow_id = e,
             id = 1:n())
})%>%
  bind_rows()%>%
  arrange(- !!sym(outcome_var))%>%
  filter(model == "Test")

final<-
  lapply(df_wf_set$wflow_id, function(e){
  error_crossmodel%>%
    mutate(m = e)%>%
    group_by(id)%>%
    mutate(val = error[wflow_id == e])%>%
    group_by(m,wflow_id)%>%
    summarise(mae = Metrics::mae(error,val),
              rmse = Metrics::rmse(error,val),
              cor = cor(error,val))
  })%>%
  bind_rows()%>%
  dplyr::select(m, wflow_id, cor)%>%
  spread(wflow_id, cor)%>%
  arrange(m)

FormatReactable(final)

gg1<-error_crossmodel%>%
  group_by(id)%>%
  mutate(max_error = max(error),
         min_error = min(error))%>%
    ggplot()+
    geom_point(aes(x = !!sym(outcome_var), y = error, colour = wflow_id), 
               alpha = 0.5, size = 0.5)+
   geom_segment(aes(y = max_error, yend= min_error,x = !!sym(outcome_var),xend=!!sym(outcome_var), group = id),
                linewidth =0.01)+
    geom_abline(slope = 0, colour = "red")+
    facet_wrap(~model, nrow = 1, scales = "free_x")+
    scale_colour_iwanthue()+
    labs(x = "Actual Value")

# Version 2
gg2<-error_crossmodel%>%
  group_by(id)%>%
  mutate(max_error = max(error),
         min_error = min(error))%>%
  ungroup()%>%
  mutate(id = fct_reorder(as.character(id), -error))%>%
    ggplot()+
    geom_point(aes(x = id, y = error, colour = wflow_id), 
               alpha = 0.5, size = 0.5)+
   geom_segment(aes(y = max_error, yend= min_error,x = id,xend= id, group = id),
                linewidth =0.01)+
    geom_abline(slope = 0, colour = "red")+
    facet_wrap(~model, nrow = 1, scales = "free_x")+
    scale_colour_iwanthue()+
    theme(axis.text.x=element_blank()) +
    labs(x = "Observation")


grid.arrange(gg1, gg2, nrow = 1)

```


```{r,cache = params$run_cache}
GenerateChunk<-
  function(nm, m) {
    template<-
        c(
          "### {{nm}}\n",
          "```{r, echo = FALSE, fig.width=12, out.width='100%', dpi=300}\n",
          #"fits[['{{nm}}_{{m}}']][[1]]%>%extract_fit_engine()%>%print()\n",
          "fits[['{{nm}}_{{m}}']][[5]]\n",
          "fits[['{{nm}}_{{m}}']][[7]]\n",
          "fits[['{{nm}}_{{m}}']][[2]]\n",
          "fits[['{{nm}}_{{m}}']][[9]]\n",
          "fits[['{{nm}}_{{m}}']][[10]]\n",
          "```\n",
          "\n"
        )

    return(knitr::knit_expand(text = template))
}
text<- c()
for (a in params$models_to_run){

  text<-c(text, paste0("## ",a, "{.tabset}"))

  # Split into all the different recipes
  text<-
    c(text,
          lapply(params$recipes_to_run, GenerateChunk, m = a)%>%unlist()
    )
}
```

`r knitr::knit(text = text)`

```{r child = file.path(params$path, "other_chunks","other_chunks.Rmd"), eval = file.exists(file.path(params$path, "other_chunks","other_chunks.Rmd"))}
```

```{r tune_xgb_test, warning=FALSE, eval = F}

# This is how to run TuneExtractModel (more for testing)

fit_xgb<-TuneExtractModel(tuned_wf =tuned_workflows,
                 string_model = "main_xgb",
                 training_data = df_train,
                 testing_data = df_test)

fit_xgb_best<- fit_xgb[[1]]
fit_xgb[[2]]

xgb_preds <-
    df_test %>%
    bind_cols(predict(fit_xgb_best, df_test))


grid.arrange(
  fit_xgb_best%>%
    pull_workflow_fit()%>%
    vip(geom = "col")
)

```

```{r compare_xgb_rf, warning=FALSE, eval=F}

#Compare Models

# TODO: Turn into mini widget where you can compare the predictions of two models

xgb_rf<-
xgb_preds%>%
  rename("xgb_pred" = .pred)%>%
  mutate(xgb_error = !!sym(outcome_var) - xgb_pred )%>%
  left_join(rf_preds%>%dplyr::select(id_hash, "rf_pred" = .pred), by = "id_hash")

xgb_rf%>%
    ggplot(aes(x = xgb_pred, y = rf_pred, colour = xgb_error))+
    geom_point()+
    geom_abline(linetype = "dashed",
                color = "gray")+
    scale_colour_distiller(type = "div")

xgb_rf%>%
    ggplot(aes(x = time_harvest_year, y = !!sym(outcome_var), colour = xgb_error))+
    geom_point()+
    geom_abline(linetype = "dashed",
                color = "gray")+
    scale_colour_distiller(type = "div")


#### Test Errors ---------------------
#Difference in test error

  # What do all the ensemble observaitons looks like compared to individual models??
  error_df<-
    xgb_rf%>%
    mutate(rf_error = !!sym(outcome_var) - rf_pred)%>%
  left_join(ensemble_preds%>%dplyr::select(id_hash, "ensemble_pred" = .pred, ensemble_error), by = "id_hash")%>%
  arrange(-ensemble_error)%>%
  # Set order of plot to
  mutate(id_hash = factor(id_hash, levels = id_hash))%>%
  gather(var, val, contains("_error"))


# Error widget

    GenerateErrorBars(error_df, "total_emissions")
    GenerateErrorBars(error_df, "ipcc_climate")
    # GenerateErrorBars(error_df, "crop")
    # GenerateErrorBars(error_df, "rs_yield")


    

WidgetErrorBar = function(dataset) {

  library(shiny)
  vars = colnames(dataset)

  shinyApp(
    ui = fluidPage(
      fluidRow(style = "padding-bottom: 20px;",
        column(12, selectInput('fill_var', 'Fill Variable', vars, selected = "total_emissions"))
      ),
      fluidRow(
        plotlyOutput('error_bar_plot', height = "400px")
      )
    ),

    server = function(input, output, session) {

      # Combine the selected variables into a new data frame

      output$error_bar_plot = renderPlotly({
        ggplotly(GenerateErrorBars(error_df, input$fill_var))
      })
    },

    options = list(height = 500)
  )
}

# WidgetErrorBar(error_df)

```