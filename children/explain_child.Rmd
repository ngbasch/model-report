---
title: "explain_child"
output: html_document
---


# Explain {.tabset}

Below is a set of diagnostics aimed to make our models more explainable. Note here that these are developed using the `DALEX` package.

For more information on Explainable ML, see the following resources:

-   [Explanatory Model Analysis (Dalex)](https://ema.drwhy.ai/) by *Przemyslaw Biecek and Tomasz Burzykowski*

-   [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/) by *Christoph Molnar*

-   [Hands on Machine Learning](https://bradleyboehmke.github.io/HOML/) by *Bradley Boehmke*

## Global {.tabset}

The following diagnostics are at the **dataset (global)** level, meaning they average behavior at the highest level of the machine learning algorithm. These methods are model-agnostic that describe average behavior of the model.

Note: All Global diagnostic plots from the `DALEX` package below are based on plugging in the training data (as opposed to the test data).

### Model Diagnostics

```{r explainer_dataset, warning=FALSE, eval=params$run_explainer,cache = params$run_cache,fig.show="hold", fig.width=10,out.width="50%", dpi=300}


# Model performance
lapply(params$models_to_explain, function(e){

  explainer<-fits[[e]][[6]]
  model_performance(explainer)

})%>%
  plot(geom = "histogram")
  #plot(resids_rf, resids_xgb, geom = "boxplot")


lapply(params$models_to_explain, function(e){

  explainer<-fits[[e]][[6]]
  model_diagnostics(explainer)
})%>%
  plot(variable = "y", yvariable = "residuals")




```

### Variable Importance

```{r explainer_vip, warning=FALSE, eval=params$run_explainer,cache = params$run_cache,fig.width=10,out.width="100%", dpi=300}
# Dalex variable importance
v<-
  lapply(params$models_to_explain, function(e){

  explainer<-fits[[e]][[6]]
  variable_importance(explainer,  loss_function = loss_root_mean_square, B = 1)

})


v%>%plot(max_vars = 15)



model_vars <- v%>%
  bind_rows()%>%
  group_by(label)%>%
  filter(dropout_loss != dropout_loss[variable == "_full_model_"])%>%
  filter(variable!="_baseline_")%>%
  arrange(-dropout_loss)%>%
  pull(variable)%>%
  unique()

top_three_most_important_vars<-model_vars[1:3]

# single_variable(explainer_rf, variable = "rs_yield")

```

### Dependence Profiles

Below is a set of plots defined below:

-   Partial Dependence Profile: "Let me show you what the model predicts on average when each data instance has the value v for that feature. I ignore whether the value v makes sense for all data instances."
-   Local Dependence Profile: "Let me show you what the model predicts on average for data instances that have values close to v for that feature. The effect could be due to that feature, but also due to correlated features."
-   Accumulated Dependence Profile: "Let me show you how the model predictions change in a small"window" of the feature around v for data instances in that window."

Source: <https://christophm.github.io/interpretable-ml-book/ale.html#motivation-and-intuition>

```{r dependence_profiles, warning=FALSE,message=FALSE, eval=params$run_explainer,cache = params$run_cache,fig.width=12, out.width="100%"}


#PDP
p1<-lapply(params$models_to_explain, function(e){

  explainer<-fits[[e]][[6]]
  model_profile(explainer, type = "partial",N = 50, variables = model_vars)

})%>%
  plot()

# Local Dependence Profiles
p2<-lapply(params$models_to_explain, function(e){

  explainer<-fits[[e]][[6]]
  model_profile(explainer, type = "conditional", N = 50, variables = model_vars)

})%>%
  plot()+
  ggtitle("Local Dependence profile")

# Accumulated Dependence Profiles
p3<-lapply(params$models_to_explain, function(e){

  explainer<-fits[[e]][[6]]
  model_profile(explainer, type = "accumulated", N = 50, variables = model_vars)

})%>%
  plot()

(p1 | p2 | p3)



# All 3 together
#TODO: Use objects from previous
lapply(params$models_to_explain, function(e){

  explainer<-fits[[e]][[6]]
  a<-model_profile(explainer, type = "accumulated", N = 50, variables = model_vars)
  a$agr_profiles$`_label_` = "accumulated local"

  b<-model_profile(explainer, type = "conditional", N = 50, variables = model_vars)
  b$agr_profiles$`_label_` = "local dependence"

  c<-model_profile(explainer, type = "partial", N = 50, variables = model_vars)
  c$agr_profiles$`_label_` = "partial dependence"

  plot(a,b,c)+
    ggtitle(paste0("Dependence Profiles (",e ,")"), subtitle = "")

})%>%
    grid.arrange(grobs = ., nrow = 1)


# plot(pdp_rf, variable_type = "categorical")




```

## Local {.tabset}

The following diagnostics are at the **instance (local)** level, meaning they are at the prediction **level** of the machine learning algorithm. These methods are useful to break down and understand model predictions for a given data point.

In our diagnostics below, we will use the following data point(s):

```{r obs, warning=FALSE, eval=params$run_explainer,cache = params$run_cache}

# Choose an observation of interest
new_obs<-df_test%>%head(1)#%>%dplyr::select(predict_vars)

FormatReactable(new_obs)
```

### Break Down Plot

Note that the ordering of breakdown plots matters.

```{r breakdown, warning=FALSE, eval=params$run_explainer,cache = params$run_cache, fig.show="hold", fig.width=10,out.width="50%",dpi=300}

# Break down plot
# Note that the ordering of breakdown matters and these are additive.

# lapply(params$models_to_explain, function(e){
# 
#   explainer<-fits[[e]][[6]]
#   predict_parts(explainer,
#                 new_observation = new_obs,
#                 type = "break_down")%>%
#     plot()
# })%>%
#   grid.arrange(grobs = .)

# Break down plot (with distributions)
lapply(params$models_to_explain, function(e){

  explainer<-fits[[e]][[6]]
  predict_parts(explainer,
                new_observation = new_obs,
                type = "break_down",
                keep_distributions = T,
                order = top_three_most_important_vars)%>%
    plot(plot_distributions = TRUE)
})%>%
  grid.arrange(grobs = .)



# Break down interactions
# Ordering also really matters here.
# lapply(params$models_to_explain, function(e){
#
#   explainer<-fits[[e]][[6]]
#   predict_parts(explainer,
#                 new_observation = new_obs,
#                 type = "break_down_interactions",
#                 interaction_preference = 2)%>%
#     plot()
# })%>%
#   grid.arrange(grobs = .)



# Shapley
# shap_rf <- predict_parts(explainer_rf, new_observation = new_obs, type = "shap", B = 50)
# plot(shap_rf)

```

### Stability

```{r stability, warning=FALSE, eval=params$run_explainer,cache = params$run_cache, fig.show="hold", fig.width=10,out.width="50%",dpi=300}

# Ceteris Paribus Profile
# lapply(params$models_to_explain, function(e){
#
#   explainer<-fits[[e]][[6]]
#   predict_profile(explainer,
#                   new_observation = new_obs)
# })%>%
# plot(variables = c("rs_yield"),
#      color = "_label_"
#      )+
#   ggtitle("Ceteris-paribus profile", "")


# plot(cp_rf, variables = c("hist_prevcrop"),
#      variable_type = "categorical", categorical_type = "bars")
#   ggtitle("Ceteris-paribus profile", "")


#Local stability
p1<-lapply(params$models_to_explain, function(e){

  n<- 30

  explainer<-fits[[e]][[6]]
  predict_diagnostics(explainer,
                         new_obs,
                         neighbors = n)%>%
    plot()+
    ggtitle(paste0("Distribution of Residuals: ", e))
})%>%
  grid.arrange(grobs = .)

# Variable specific stability
p2<-lapply(params$models_to_explain, function(e){

  explainer<-fits[[e]][[6]]
  predict_diagnostics(explainer,
                         new_obs,
                         neighbors = 30,
                      variables = top_three_most_important_vars[1])%>%
    plot()
})%>%
  grid.arrange(grobs = .)

# grid.arrange(p1, p2, nrow = 1)
```

```{r get_feature_contrib, echo=FALSE, eval = F}


x<- GetFeatureContribData(df_test, explainer_rf)


x%>%
  mutate(nrow = 1:n())%>%
  filter(!(variable_name %in% c("prediction", "predicted_emissions_w_adjustments")))%>%
  ggplot(aes(y = variable_name, x = contribution, fill = variable_name))+
  geom_boxplot() +
  #  geom_density_ridges(alpha = 0.5) +
  scale_fill_iwanthue() +
  labs(y = "")+
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  )

hist_crop_mapping<-
  tibble(crop = levels(emice_complete$hist_prevcrop),
         n = as.character(1:length(levels(emice_complete$hist_prevcrop))),
  )

# Factor variable break down
x%>%
  mutate(nrow = 1:n())%>%
  filter(variable_name %in% c("hist_prevcrop", "hist_prev2crop"))%>%
  left_join(hist_crop_mapping, by = c("variable_levels" = "n"))%>%
  ggplot(aes(y = crop, x = contribution, fill = variable_name))+
  #  geom_boxplot() +
  geom_density_ridges(alpha = 0.5) +
  scale_fill_iwanthue() +
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  ) +
  facet_wrap(~variable_name, ncol = 1)

```
